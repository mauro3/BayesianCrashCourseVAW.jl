---
title: "Crash course in Bayesian inversion"
bibliography: 2024-Bayesian-Crash-Course-VAW.bib
link-citations: true
csl: journal-of-glaciology.csl
execute:
  enabled: true
engine: julia
julia:
  exeflags: ["--project=@."]
format:
  revealjs:
    incremental: false
    logo: figs/wsl-eth-logo.png
    theme: moon
    css: logo.css 
    smaller: true
author: "Mauro Werder"
---

# Bayesian what?

## Bayesian inversion
**Bayesian inversion** is a statistical method for estimating unknown model parameters including their uncertainty.

Using Bayes' theorem, it fits a forward model to observations whilst also taking into account prior information.

## Examples: @Brinkerhoff.etal2021

:::: {.columns}

::: {.column width="40%"} 

::: {style="font-size: 100%;"}
Looks at coupled dynamics of ice flow and subglacial drainage

- first study of a coupled ice-flow (lastest generation) drainage system model inversion
- uses a surrogate forward model
- Bayesian inversion based around MCMC (Markov chain Monte Carlo)
:::
:::

::: {.column width="60%"}
![Fig: Model region in West Greenland]( ./figs/brinkerhoff2022-1.png )
:::

::::

## Examples: @Brinkerhoff.etal2021

:::: {.columns}
::: {.column width="40%"} 
::: {style="font-size: 100%;"}
- inversion estimates eight parameters
- and their distributions

[explain what that plot shows]
:::
:::

::: {.column width="60%"}
![]( ./figs/brinkerhoff2022-2.png )
:::
::::

## Examples: @Werder.etal2020

:::: {.columns}
::: {.column width="40%"} 
::: {style="font-size: 100%;"}
Invert for ice thickness given radar and flow speed observations:

- uses the @Farinotti.etal2009b ice thickness model as forward model
- fits observations of thickness and flow speed by tuning various parameters
- this is what got me started with Bayesian inference / inversions
:::
:::
::: {.column width="60%"}
![Fig: inversion for Unteraargletscher]( figs/werder2020-1.png )
:::
::::
 
## Examples: @Werder.etal2020
![Fig: inversion for Unteraargletscher: comparison to data]( figs/werder2020-2.png )

## Examples: @Pohle.etal2022
:::: {.columns}
::: {.column width="40%"} 
::: {style="font-size: 100%;"}
Invert for parameters of R-channel model given our measurements

- forward models: two simple R-channel models
- one is fitted, one errors are just forward propagated
- fit to channel size S
- fit with temperature gradient and initial S
- comparison between the predicted temperature gradients 
:::
:::
::: {.column width="60%"}
![]( figs/pohle2022-1.png )
:::
::::

## Disclaimer

- I am not a specialist on this, let alone a statistician, merely a user
- this presentation is a bit less clear than I was hoping... good luck!

# Terminology

## Terminology: Uncertainty vs. Errors

- **Error**: discrepancy between model / measurement and reality
  - typically we don't actually know the error (as the truth is unknown)
  - examples: 
    - data error: instrument precision limits, noise, etc
    - numerical error: difference between numerical model result and true solution
- **Uncertainty**: lack of complete knowledge about a system or its parameters
  - typically given as a probability distribution
  - errors can be conceptualized as random draws from this distribution

A **probabilistic framework** quantifies uncertainty, enabling us to model errors probabilistically.

## Terminology: Types of Uncertainty
::: {.columns}
::: {.column width="50%"}
### **Aleatoric Uncertainty**
- stochastic variability inherent to a process
- also called **irreducible uncertainty**
- Examples:
  - atmospheric variability
  - sensor noise
  - quantum fluctuations
:::
::: {.column width="50%"}

### **Epistemic Uncertainty**
- due to lack of knowledge
- also called **systematic uncertainty**
- can be reduced **with better data or models**
- examples:
  - poorly constrained model parameters
  - sparse glacier field measurements
:::
:::

. . .

 Ideally we want to quantify both types...

## Terminology: forward vs inverse models

A **forward model** is a model as we know and love it: 

- feed it input and it spits out a result
- typically that is the kind of model we create when envisaging to model a process

An **inverse model** is the combination of:

- a forward model
- and a way to fit that model's output to observations / data

# Forward uncertainty propagation

## When is forward uncertainty propagation enough?

Forward propagation is the only possibility when:

- model input parameters and their uncertainty are know or can be estimated (guesses, expert judgement,etc.)
- there are no observation/data to fit to the outputs of the forward model

In this case, the focus is on propagating known input uncertainty through the forward model to quantify **uncertainty in the outputs**.

## Classical Error Propagation

- Assumes **small, uncorrelated uncertainties** and **linear relationships**.
- Uses **partial derivatives** to propagate input uncertainty through the model.
  
**1D Example:**

For a function $y = f(x)$, where $\sigma_x$ is the uncertainty in $x$:

$\sigma_y(x_0) = \frac{\partial f}{\partial x} \Big|_{x=x_0} \cdot \sigma_x(x_0)$\  

**Example:**
If $y = x^2$ and $\sigma_x = 0.1$:

$\sigma_y = 2x \cdot \sigma_x = 0.2 x$

. . .

**Note:** this linearises the function and thus is only valid for linear functions or close the the point of evaluation.

## Monte Carlo Uncertainty Propagation

Monte Carlo (MC) methods propagate uncertainty by creating many input parameter sets by **sampling the input's distributions** and running the forward model for each set.

<br>

**Key Steps:**

1. define **uncertainty distributions** for each input parameter
   - example: $\mathcal{N}(\mu, \sigma)$, $\text{Uniform}(a, b)$
2. **randomly sample** input parameters from these distributions
3. run the forward model to calculate the corresponding outputs
4. analyze the **distribution of the outputs** for uncertainty in results


**Pros:**

- handles **nonlinear models** and correlated inputs
- any form of input distribution (not limited to Gaussian)


## **Julia Code Example: Monte Carlo Uncertainty Propagation**

A glacier mass-balance model calculates annual mass balance at a location on the glacier, $MB$. 
In gereral it would look something like:

$MB(P, T, m, ...) = f(P, T, m, ..)$

where $f$ is some, maybe complicated, function.  

<br>

::: {.fragment}
For now we take this simple model:

  $MB = P + T \cdot m$
  
where:

  - $P$: solid precipitation (uncertain, modeled as $\mathcal{N}(1000, 100)$).
  - $T$: mean annual temperature (uncertain, $\mathcal{N}(2, 0.5)$).
  - $m$: melt factor ($10.0$, fixed).
:::
---

**Julia implementation**

```{julia}
#| label: monte-carlo-propagation
#| echo: true  # Show full code and comments
using Distributions, Random, Statistics, CairoMakie # for stats and plotting

# Define the forward model
glacier_mb(P, T, m) = P - T*m

# Define input distributions
P_dist = Normal(1000, 100)  # Snow precipitation distribution (mean=1000mm, std=100mm)
T_dist = Normal(2, 0.5)     # Annual temperature distribution (mean=2°C, std=0.5°C)
# Fixed melt factor
m = 200.0

# Monte Carlo sampling
num_samples = 10000
P_samples = rand(P_dist, num_samples)  # Sample precipitation
T_samples = rand(T_dist, num_samples)  # Sample temperature

# Compute mb for each sample
mb_samples_MC = [glacier_mb(P, T, m) for (P, T) in zip(P_samples, T_samples)]

# Analyze results
mean_mb, std_mb  = mean(mb_samples_MC), std(mb_samples_MC)
# Print results
println("\nMean annual mass balance: $(mean_mb) mm")
println("Standard deviation of MB: $(std_mb) mm")
```

---

**Output plot**

```{julia}
#| fig-cap: "Distribution of simulated melt"
#| fig-alt: "A Gaussian shaped histogram"
#| echo: true  # Show full code and comments
# Plot results
fig = Figure()
ax = Axis(fig[1, 1], title="Monte Carlo Mass Balance Distribution", xlabel="Mass balance (mm)", ylabel="Frequency")
hist!(ax, mb_samples_MC, bins=50, color=:blue, strokewidth=0.0,); fig
```

# Inversions
## When do we need inversions?

Essentially, when our model produces output for which we have measurements and we want to use that to constrain the model parameters.

- "parameters" is used here in the sense of *any model input*
  - for example: ice density, DEM, numerical stuff, magic constants, etc
  - note that in the scheme presented here, any parameters which have uncertainty need to be fitted; i.e. no "straight" MC propagation of errors
- inversions are in general ill posed and need regularisation.  This is done in Bayesian inversion with so-called priors (but they can be used for other things too)

## Prior knowledge, aka "Priors"
Before we start to run our inversion we typically already have some knowledge of the model parameters:

- direct measurements of a parameter, e.g. the DEM and its uncertainty
- inversions of a parameter from a previous study
- physics and other fundamental constraints
- expert opinion...

. . .

<br>
Note that this is what we used in the uncertainty forward propagation example: just our prior knowledge of the parameters. 

## Prior: $p(\theta)$

Formally, priors are written as

$p(\theta)$

where $p$ is a probability density function (PDF) and $\theta$ is the vector of parameters.

. . .

<br>
So for our previous example:

$\theta = [P, T]$,

remember $m$ was fixed, $P\sim N(1000,100)$ and $T\sim N(2,0.5)$.

. . .

And as our normal priors on $P$ and $T$ are uncorrelated the PDF of the prior is:

$p(\theta) = \frac{1}{\sqrt{2\pi100^2}} e^{-\frac{(\theta_P-1000)^2}{2\cdot 100^2}} \cdot \frac{1}{\sqrt{2\pi0.5^2}} e^{-\frac{(\theta_T-2)^2}{2 \cdot 0.5^2}}$.

## Forward model and how to test it with data, aka the likelihood

Going back to our mass balance model: it has the form

$MB(T,P,m,...) = f(T,P,m,...)$ 

(where above $m$ was super simple.  However $f$ may well be more complicated, say taking DEMs, wind fields, etc as inputs.)

. . .

Now we have data for $MB$ from direct glaciological observations: **how do we incorporate that data?**

. . .

--> Fit the model to the data.  

- For this we need a cost-function, objective-function, etc.  In Bayesian inversion this is the **likelihood**.

## Likelihood: $p(d|\theta)$

To compare the data to the forward model output, an assumption on how the errors are distributed.
This is captured in the likelihood $p(d|\theta)$, which reads something as:

*"the likelihood of measuring the data $d$ given our forward model run with parameters $\theta$ and a stochastic model of the errors"*

. . .

Usually, for lack of better knowledge, a normal distribution is assumed for the errors:

$p(d | \theta) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma_i^2}} e^{-\frac{(F(\theta)_i - d_i)^2}{2\sigma_i^2}}$

for the case of $n$ uncorrelated errors, where $F(\theta)$ is the forward model.

. . .

<br>
For our example this would be 

$p(d | \theta) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(MB(\theta)_i - d)^2}{2\sigma^2}}$

with a single observation of the mass-balance $d$.

## Bayes' theorem
### Combining prior and knowledge gained from modelling

We now want to combine the prior $p(\theta)$ and the likelihood $p(d|\theta)$... Bayes' theorem to the rescue!

Stated in terms of PDFs:

$p(\theta|d) = \frac{p(d|\theta) \cdot p(\theta)}{p(d)}$

where you will spot the prior and likelihood as well as the marginal likelihood $p(d)$ and the posterior $p(\theta|d)$.

<br>

:::{.fragment}
**The posterior is what we are after**, it tells us the probability of a set of parameters being appropriate given our prior knowledge, the forward model and an error model.
:::

## Worked example
We will now continue with our simple mass balance example from before (this uses a package for more compact code, we'll do it from scratch this afternoon).

```{julia}
#| label: bayes-inversion
#| echo: true  # Show full code and comments
using Turing # using a MCMC package, and plotting

@model function bayesian_glacier_model(mb_measured, mb_uncertainty)
    # Priors
    P ~ Normal(1000, 100)  # Precipitation
    T ~ Normal(2, 0.5)     # Temperature
    # Fixed melt factor
    m = 200.0
    
    # Likelihood
    mb = P - T * m
    mb_measured ~ Normal(mb, mb_uncertainty)
end

# Define model and draw samples from the posterior
model = bayesian_glacier_model(400.0, 100.0)
chain = sample(model, NUTS(0.65), 1000, progress=false);
summarystats(chain)
```

---

```{julia}
#| fig-cap: "Prior (green) vs posterior (blue) distirbution of P (left) and T (right)"
#| echo: false  # Show full code and comments
# Plot prior vs posterior distributions
# Extract samples for precipitation (P) and temperature (T)
P_samples, T_samples = chain[:P][:], chain[:T][:] # Samples of P and T
# make samples from the prior to compare
P_samples_prior,T_samples_prior = rand(Normal(1000, 100), 10000), rand(Normal(2, 0.5), 10000)

# Plot precipitation posterior
fig = Figure(); ax1 = Axis(fig[1, 1], title="Posterior Distribution: Precipitation (P)", xlabel="P (mm)", ylabel="Density")
hist!(ax1, P_samples_prior, bins=50, normalization=:pdf, color=:green, strokewidth=0.0)
hist!(ax1, P_samples, bins=50, normalization=:pdf, color=:blue, strokewidth=0.0)
# Plot temperature posterior
ax2 = Axis(fig[1, 2], title="Posterior Distribution: Temperature (T)", xlabel="T (°C)", ylabel="Density")
hist!(ax2, T_samples_prior, bins=50, normalization=:pdf, color=:green, strokewidth=0.0)
hist!(ax2, T_samples, bins=50, normalization=:pdf, color=:blue, strokewidth=0.0); fig
```

The histograms show how our prior knowledge of precipitation and temperature is updated by the fitting to the measured mass balance.

## Forecasts

We can also look at what the model then "forecasts" for the actual value of the mass balance, taking prior knowledge into account as well.
```{julia}
#| fig-cap: "Prior (green) vs posterior (blue) distirbution of mass balance"
#| echo: false  # Show full code and comments
# Compute mass balance for each posterior sample
using Colors
mb_samples = glacier_mb.(P_samples, T_samples, m)[:]
fig = Figure(); ax = Axis(fig[1, 1], title="Mass Balance Posterior Distribution",
          xlabel="Mass Balance (mm)", ylabel="Density")
hist!(ax, mb_samples_MC, bins=50, normalization=:pdf, color=:green, strokewidth=0.0)
hist!(ax, mb_samples, bins=50, normalization=:pdf, color=:blue, strokewidth=0.0); fig
```

## Joint probabilities

```{julia}
# Define labels and pairwise combinations
variables = ["P (Precipitation)", "T (Temperature)", "mb (Mass Balance)"]
samples = (P_samples, T_samples, mb_samples)

# Compute correlation matrix
correlations = [cor(samples[i], samples[j]) for i in 1:3, j in 1:3]  # Pairwise correlations

# Create a grid figure for a 3x3 matrix
fig = Figure()

# Loop over each plot position in the 3x3 grid
for i in 1:3
    for j in 1:3
        ax = fig[i, j] = Axis(fig[i, j], 
            xlabel=i == 3 ? variables[j] : "",  # Add xlabel to the bottom row
            ylabel=j == 1 ? variables[i] : "",  # Add ylabel to the left column
            xticklabelsize=8,
            yticklabelsize=8)

        if i == j
            # Diagonal: Marginal distribution (histogram)
            hist!(ax, samples[i], bins=50, normalization=:pdf, color=:blue, strokewidth=0.0)
        elseif i > j
            # Lower triangle: Pairwise joint distribution (scatter plot)
            scatter!(ax, samples[j], samples[i], color=:blue, markersize=2)
        else
            # Upper triangle: Correlation coefficient
            corr_val = round(correlations[i, j], digits=2)
            text!(ax, 0.5, 0.5, text=string(corr_val), fontsize=20, color=:black)

           # Add colored background
           if corr_val > 0  # Positive correlation: Green
              poly!(ax, [0, 1, 1, 0], [0, 0, 1, 1], color=RGBA(0.8, 1.0, 0.8, abs(corr_val)))
           else  # Negative correlation: Red
              poly!(ax, [0, 1, 1, 0], [0, 0, 1, 1], color=RGBA(1.0, 0.8, 0.8, abs(corr_val)))
           end

            hidedecorations!(ax)  # Hide ticks and labels in the upper triangle
        end
    end
end

fig
```
The correlations are as expected, postive between T-mb and T-P and negative T-mb.  

## Methods for doing Bayesian inversions

**Sampling**, why sampling?

- a sampler takes a PDF and returns a sample from it. The `rand` command is an example sampling from `Uniform(0,1)`
- good to characterize high dimensional probability distributions via histograms and scatter plots
- integration, which is needed for expectation values, etc., is just a summation away

. . .

**Maximum posterior estimates**

- instead of sampling the posterior to get the full distribution, just finding the maximum can also be done
- uses gradient ascent or similar and probably faster than sampling as less model evaluations are needed
- this is largely equivalent to other types of inversions where a cost function (likelihood) with some regularisation (prior) is minimized

## What is that MCMC stuff?

Sampling is important not least because it can be done very efficiently via Markov chain Monte Carlo (MCMC) methods.  Thus MCMC and Bayesian inversion often is used almost interchangeably; but MCMC is really just the method used to sample.

. . .

**Metropolis Hastings** is the original MCMC sampler

- in the example above, this "Turing"-thing uses an MCMC sampler
- we will look at MH this afternoon
- but nowadays often more modern samplers are used.  In particular Hamiltonian Monte Carlo methods are often used, but require calculation of the gradient of the posterior
- we will use an affine invariant sampler, which is what I used in most of my work

## Things not covered

::: {.incremental}
- the construction of the likelihood and priors are not always trivial, but I get away with being not good at it
  - correlations should be accounted for, e.g. spatial, to ensure smoothness, avoid overfitting and not overestimate measurement contributions
  - non informative priors: how to choose a prior such that it has no impact on the results
- there are packages out there to do Bayesian inference, I find them impenetrable because the use some much stats jargon...
- yes, stats jargon! Can't help you there. If you get dizzy because of it, focus on your forward model and wait for it to pass.
- probably other things which I'm not even aware of
:::

## Summary
:::: {.columns}
::: {.column width="50%"} 
![Brinkerhoff et al., 2022]( ./figs/brinkerhoff2022-2.png )
:::
::: {.column width="50%"} 
::: {style="font-size: 90%;"}
::: {.incremental}
- taking uncertainties into account is important
- forward propagation of uncertainties is often good enough (i.e. no need to fit a model)
  - use MC scheme
- Bayesian inference is a nice inversion scheme
  - an MCMC-based approach will give the best fit as well as uncertainties
  - however, maximum posterior evaluation can also be done; this is then pretty equivalent to other inversions
- the plan for this afternoon is to code such an inversion scheme
:::
:::
:::
::: {.fragment}
 Thanks!
:::
::::
---

# References
