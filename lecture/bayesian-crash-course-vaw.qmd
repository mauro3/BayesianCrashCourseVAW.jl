---
title: "Crash course in Bayesian inversion"
bibliography: 2024-Bayesian-Crash-Course-VAW.bib
link-citations: true
csl: journal-of-glaciology.csl
execute:
  enabled: true
engine: julia
julia:
  exeflags: ["--project=@."]
format:
  revealjs:
    incremental: false
    logo: figs/wsl-eth-logo.png
    theme: moon
    css: logo.css 
    smaller: true
author: "Mauro Werder"
---

# Bayesian what?

## Bayesian inversion
**Bayesian inversion** is a statistical method for estimating unknown model parameters including their uncertainty.

Using Bayes' theorem, it fits a forward model to observations whilst also taking into account prior information.

## Examples: @Brinkerhoff.etal2021

:::: {.columns}

::: {.column width="40%"} 

::: {style="font-size: 100%;"}
Looks at coupled dynamics of ice flow and subglacial drainage

- first study of a coupled ice-flow (lastest generation) drainage system model inversion
- uses a surrogate forward model
- Bayesian inversion based around MCMC (Markov chain Monte Carlo)
:::
:::

::: {.column width="60%"}
![Fig: Model region in West Greenland]( ./figs/brinkerhoff2022-1.png )
:::

::::

## Examples: @Brinkerhoff.etal2021

:::: {.columns}
::: {.column width="40%"} 
::: {style="font-size: 100%;"}
- inversion estimates eight parameters
- and their distributions

[explain what that plot shows]
:::
:::

::: {.column width="60%"}
![]( ./figs/brinkerhoff2022-2.png )
:::
::::

## Examples: @Werder.etal2020

:::: {.columns}
::: {.column width="40%"} 
::: {style="font-size: 100%;"}
Invert for ice thickness given radar and flow speed observations:

- uses the @Farinotti.etal2009b ice thickness model as forward model
- fits observations of thickness and flow speed by tuning various parameters
- this is what got me started with Bayesian inference / inversions
:::
:::
::: {.column width="60%"}
![Fig: inversion for Unteraargletscher]( figs/werder2020-1.png )
:::
::::
 
## Examples: @Werder.etal2020
![Fig: inversion for Unteraargletscher: comparison to data]( figs/werder2020-2.png )

## Examples: @Pohle.etal2022
:::: {.columns}
::: {.column width="40%"} 
::: {style="font-size: 100%;"}
Invert for parameters of R-channel model given our measurements

- forward models: two simple R-channel models
- one is fitted, one errors are just forward propagated
- fit to channel size S
- fit with temperature gradient and initial S
- comparison between the predicted temperature gradients 
:::
:::
::: {.column width="60%"}
![]( figs/pohle2022-1.png )
:::
::::

## Disclaimer

- I am not a specialist on this, let alone a statistician, merely a user
- this presentation is a bit less clear than I was hoping... good luck!

# Terminology

## Terminology: Uncertainty vs. Errors

- **Error**: discrepancy between model / measurement and reality
  - typically we don't actually know the error (as the truth is unknown)
  - examples: 
    - data error: instrument precision limits, noise, etc
    - numerical error: difference between numerical model result and true solution
- **Uncertainty**: lack of complete knowledge about a system or its parameters
  - typically given as a probability distribution
  - errors can be conceptualized as random draws from this distribution

A **probabilistic framework** quantifies uncertainty, enabling us to model errors probabilistically.

## Terminology: Types of Uncertainty
::: {.columns}
::: {.column width="50%"}
### **Aleatoric Uncertainty**
- stochastic variability inherent to a process
- also called **irreducible uncertainty**
- Examples:
  - atmospheric variability
  - sensor noise
  - quantum fluctuations
:::
::: {.column width="50%"}

### **Epistemic Uncertainty**
- due to lack of knowledge
- also called **systematic uncertainty**
- can be reduced **with better data or models**
- examples:
  - poorly constrained model parameters
  - sparse glacier field measurements
:::
:::

. . .

 Ideally we want to quantify both types...

## Terminology: forward vs inverse models

A **forward model** is a model as we know and love it: 

- feed it input and it spits out a result
- typically that is the kind of model we create when envisaging to model a process

An **inverse model** is the combination of:

- a forward model
- and a way to fit that model's output to observations / data

# Forward uncertainty propagation

## When is forward uncertainty propagation enough?

Forward propagation is the only possibility when:

- model input parameters and their uncertainty are know or can be estimated (guesses, expert judgement,etc.)
- there are no observation/data to fit to the outputs of the forward model

In this case, the focus is on propagating known input uncertainty through the forward model to quantify **uncertainty in the outputs**.

## Classical Error Propagation

- Assumes **small, uncorrelated uncertainties** and **linear relationships**.
- Uses **partial derivatives** to propagate input uncertainty through the model.
  
**1D Example:**

For a function $y = f(x)$, where $\sigma_x$ is the uncertainty in $x$:

$\sigma_y(x_0) = \frac{\partial f}{\partial x} \Big|_{x=x_0} \cdot \sigma_x(x_0)$\  

**Example:**
If $y = x^2$ and $\sigma_x = 0.1$:

$\sigma_y = 2x \cdot \sigma_x = 0.2 x$

. . .

**Note:** this linearises the function and thus is only valid for linear functions or close the the point of evaluation.

## Monte Carlo Uncertainty Propagation

Monte Carlo (MC) methods propagate uncertainty by creating many input parameter sets by **sampling the input's distributions** and running the forward model for each set.

<br>

**Key Steps:**

1. define **uncertainty distributions** for each input parameter
   - example: $\mathcal{N}(\mu, \sigma)$, $\text{Uniform}(a, b)$
2. **randomly sample** input parameters from these distributions
3. run the forward model to calculate the corresponding outputs
4. analyze the **distribution of the outputs** for uncertainty in results


**Pros:**

- handles **nonlinear models** and correlated inputs
- any form of input distribution (not limited to Gaussian)


## **Julia Code Example: Monte Carlo Uncertainty Propagation**

A glacier mass-balance model calculates annual mass balance at a location on the glacier, $MB$. 
In gereral it would look something like:

$MB(P, T, m, ...) = f(P, T, m, ..)$

where $f$ is some, maybe complicated, function.  

<br>

::: {.fragment}
For now we take this simple model:

  $MB = P + T \cdot m$
  
where:

  - $P$: solid precipitation (uncertain, modeled as $\mathcal{N}(1000, 100)$).
  - $T$: mean annual temperature (uncertain, $\mathcal{N}(2, 0.5)$).
  - $m$: melt factor ($10.0$, fixed).
:::
---

**Julia implementation**

```{julia}
#| label: monte-carlo-propagation
#| echo: true  # Show full code and comments
using Distributions, Random, Statistics, CairoMakie # for stats and plotting

# Define the forward model
glacier_mb(P, T, m) = P - T*m

# Define input distributions
P_dist = Normal(1000, 100)  # Snow precipitation distribution (mean=1000mm, std=100mm)
T_dist = Normal(2, 0.5)     # Annual temperature distribution (mean=2°C, std=0.5°C)
# Fixed melt factor
m = 200.0

# Monte Carlo sampling
num_samples = 10000
P_samples = rand(P_dist, num_samples)  # Sample precipitation
T_samples = rand(T_dist, num_samples)  # Sample temperature

# Compute mb for each sample
mb_results = [glacier_mb(P, T, m) for (P, T) in zip(P_samples, T_samples)]

# Analyze results
mean_mb, std_mb  = mean(mb_results), std(mb_results)
# Print results
println("\nMean annual mass balance: $(mean_mb) mm")
println("Standard deviation of MB: $(std_mb) mm")
```

---

**Output plot**

```{julia}
#| fig-cap: "Distribution of simulated melt"
#| fig-alt: "A Gaussian shaped histogram"
#| echo: true  # Show full code and comments
# Plot results
fig = Figure()
ax = Axis(fig[1, 1], title="Monte Carlo Mass Balance Distribution", xlabel="Mass balance (mm)", ylabel="Frequency")
hist!(ax, mb_results, bins=50, color=:blue, strokewidth=0.0,); fig
```

# Inversions
## When do we need inversions?

Essentially, when our model produces output for which we have measurements and we want to use that to constrain the model parameters.

- "parameters" is used here in the sense of *any model input*
  - for example: ice density, DEM, numerical stuff, magic constants, etc
  - note that in the scheme presented here, any parameters which have uncertainty need to be fitted; i.e. no "straight" MC propagation of errors
- inversions are in general ill posed and need regularisation.  This is done in Bayesian inversion with so-called priors (but they can be used for other things too)

## Prior knowledge, aka "Priors"
Before we start to run our inversion we typically already have some knowledge of the model parameters:

- direct measurements of a parameter, e.g. the DEM and its uncertainty
- inversions of a parameter from a previous study
- physics and other fundamental constraints
- expert opinion...

. . .

<br>
Note that this is what we used in the uncertainty forward propagation example: just our prior knowledge of the parameters. 

## Prior: $p(\theta)$

Formally, priors are written as

$p(\theta)$

where $p$ is a probability density function (PDF) and $\theta$ is the vector of parameters.

. . .

<br>
So for our previous example:

$\theta = [P, T]$,

remember $m$ was fixed, $P\sim N(1000,100)$ and $T\sim N(2,0.5)$.

. . .

And as our normal priors on $P$ and $T$ are uncorrelated the PDF of the prior is:

$p(\theta) = \frac{1}{\sqrt{2\pi100^2}} e^{-\frac{(\theta_P-1000)^2}{2\cdot 100^2}} \cdot \frac{1}{\sqrt{2\pi0.5^2}} e^{-\frac{(\theta_T-2)^2}{2 \cdot 0.5^2}}$.

## Forward model and how to test it with data, aka the likelihood

Going back to our mass balance model: it has the form

$MB(T,P,m,...) = f(T,P,m,...)$ 

(where above $m$ was super simple.  However $f$ may well be more complicated, say taking DEMs, wind fields, etc as inputs.)

. . .

Now we have data for $MB$ from direct glaciological observations: **how do we incorporate that data?**

. . .

--> Fit the model to the data.  

- For this we need a cost-function, objective-function, etc.  In Bayesian inversion this is the **likelihood**.

## Likelihood: $p(d|\theta)$

To compare the data to the forward model output, an assumption on how the errors are distributed.
This is captured in the likelihood $p(d|\theta)$, which reads something as:

*"the likelihood of measuring the data $d$ given our forward model run with parameters $\theta$ and a stochastic model of the errors"*

. . .

Usually, for lack of better knowledge, a normal distribution is assumed for the errors:

$p(d | \theta) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma_i^2}} e^{-\frac{(F(\theta)_i - d_i)^2}{2\sigma_i^2}}$

for the case of $n$ uncorrelated errors, where $F(\theta)$ is the forward model.

. . .

<br>
For our example this would be 

$p(d | \theta) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(MB(\theta)_i - d)^2}{2\sigma^2}}$

with a single observation of the mass-balance $d$.

## Bayes' theorem
### Combining prior and knowledge gained from modelling

We now want to combine the prior $p(\theta)$ and the likelihood $p(d|\theta)$... Bayes' theorem to the rescue!

Stated in terms of PDFs:

$p(\theta|d) = \frac{p(d|\theta) \cdot p(\theta)}{p(d)}$

where you will spot the prior and likelihood as well as the marginal likelihood $p(d)$ and the posterior $p(\theta|d)$.

<br>

:::{.fragment}
**The posterior is what we are after**, it tells us the probability of a set of parameters being appropriate given our prior knowledge, the forward model and an error model.
:::

## Worked example

We will now continue with our simple mass balance example from before.


## Forecasts

## Summary
:::: {.columns}
::: {.column width="50%"} 
::: {.incremental}
- taking uncertainties into account is important
- forward propagation of uncertainties is often good enough (i.e. no need to fit a model)
  - use MC scheme
- Bayesian inference is a nice inversion scheme
  - an MCMC-based approach will give the best fit as well as uncertainties
  - however, maximum posterior evaluation can also be done; this is then pretty equivalent to other inversions
- the plan for this afternoon is to code such an inversion scheme
:::
:::
::: {.fragment}
 Thanks!
:::
::: {.column width="50%"} 
![]( ./figs/brinkerhoff2022-2.png )
:::
::::
---

# References
